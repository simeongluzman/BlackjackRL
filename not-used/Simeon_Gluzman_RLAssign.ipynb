{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4yvI-kf-myN"
      },
      "source": [
        "# MDPs and Q-learning On \"Ice\" (60 points possible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEuDHFs-ixs"
      },
      "source": [
        "In this assignment, we’ll revisit Markov Decision Processes while also trying out Q-Learning, the reinforcement learning approach that associates utilities with attempting actions in states.\n",
        "The problem that we’re attempting to solve is the following:\n",
        "\n",
        "1.  There is a grid of spaces in a rectangle.  Each space can contain a pit (negative reward), gold (positive reward), or nothing.\n",
        "2.  The rectangle is effectively surrounded by walls, so anything that would move you outside the rectangle, instead moves you to the edge of the rectangle.\n",
        "3.  The floor is icy.  Any attempt to move in a cardinal direction results in moving a somewhat random number of spaces in that direction.  The exact probabilities of moving each number of spaces are given in the problem description.  (If you slide too far, see rule #2.)\n",
        "4.  Landing on a pit or gold effectively “ends the run,” for both a Q learner and an agent later trying out the policy.  It’s game over.  (To simulate this during Q learning, set all Q values for the space to equal its reward, then start over from a random space.)  Note that it's still possible to slide past a pit or gold - this doesn't end the run.\n",
        "\n",
        "A sample input looks like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L_PSiQiO-_2y"
      },
      "outputs": [],
      "source": [
        "sampleMDP = \"\"\"0.7 0.2 0.1\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CmLASMB_Gsd"
      },
      "source": [
        "\n",
        "The first line says that the probabilities of moving one, two, or three spaces in the direction of movement are 0.7, 0.2, and 0.1.   The rest is a map of the environment, where a dash is an empty space, P is a pit, and G is gold.\n",
        "\n",
        "Your job is to finish the code below for mdp_solve() and q_solve().  These take a problem description like the one pictured above, and return a policy giving the recommended action to take in each empty square (U=up, R=right, D=down, L=left).\n",
        "\n",
        "**1, 17 points)**  mdp_solve() should use value iteration and the Bellman equation.  ITERATIONS will refer to the number of complete passes you perform over all states.  You can initialize the utilities to the rewards of each state.  Don’t update the rewards spaces from their initial rewards; since they end the trial, they have no future utility.  Don't update utilities in-place as you iterate through them, but create a fresh array of utilities with each pass, in order to avoid biasing moves in the directions that have already been updated.\n",
        "\n",
        "**2, 24 points)**  q_solve() will run ITERATIONS trials in which a learner starts in a random empty square and moves until it hits a pit or gold, in which case, the trial is over.  (If it was randomly dropped into gold or a pit, the trial is immediately over.)  The learner moves by deciding randomly whether to choose a random direction (with probability EXPLORE_PROB) or move according to the best Q-value of its current square (otherwise).  Simulate the results of the move on slippery ice to determine where the learner ended up - then apply the Q-learning equation given in lecture and the textbook.  (There are multiple Q-learning variants out there, so try to use the equations and practices described in lecture instead of using other sources, to avoid confusion.)\n",
        "\n",
        "The fact that a trial ends immediately on finding gold or a pit means that we want to handle those spaces in a special way.  Normally Q values are updated on moving to the next state, but we won’t see any next state in these cases.  So, to handle this, when the agent discovers one of these rewards, set all the Q values for that space to the associated reward before quitting the trial.  So, for example, if gold is worth 100 and it’s discovered in square x, Q(x,UP) = 100, Q(x,RIGHT) = 100, Q(x, DOWN) = 100, and Q(x, LEFT) = 100.  There’s no need to apply the rest of the Q update equation when the trial is ending, because that’s all about future rewards, and there’s no future when the trial is ending.  But now the spaces that can reach that space will evaluate themselves appropriately.  (Before being \"discovered,\" the square should have no utility.)\n",
        "\n",
        "You should use the GOLD_REWARD, PIT_REWARD, LEARNING_RATE, and DISCOUNT_FACTOR constants at the top of the code box below.\n",
        "\n",
        "Q-learning involves a lot of randomness and some arbitrary decisions when breaking ties, so two implementations can both be correct but recommend slightly different policies in the end, even if they have the same starting random seed.  While we provide some helpful premade maps below, your main guide for debugging will be common sense in deciding whether the policy created by your agent makes sense -- ie, agents following the policy will get gold without taking unnecessary risks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZW7aHFXpUQ9l"
      },
      "outputs": [],
      "source": [
        "\"\"\" \"MDPs on Ice - Assignment 5\"\"\"\n",
        "\n",
        "import random\n",
        "import copy\n",
        "\n",
        "GOLD_REWARD = 250.0\n",
        "PIT_REWARD = -150.0\n",
        "DISCOUNT_FACTOR = 0.8\n",
        "EXPLORE_PROB = 0.2 # for Q-learning\n",
        "LEARNING_RATE = 0.01\n",
        "ITERATIONS = 20000\n",
        "MAX_MOVES = 1000\n",
        "ACTIONS = 4\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "MOVES = ['U', 'R', 'D', 'L']\n",
        "\n",
        "# Fixed random number generator seed for result reproducibility --\n",
        "# don't use a random number generator besides this to match sol\n",
        "random.seed(340)\n",
        "\n",
        "class Problem:\n",
        "    \"\"\"Represents the physical space, transition probabilities, reward locations, and approach\n",
        "\n",
        "    ...in short, the info in the problem string\n",
        "\n",
        "    Attributes:\n",
        "        move_probs (List[float]):  probabilities of going 1,2,3 spaces\n",
        "        map (List[List(string)]]:  \"-\" (safe, empty space), \"G\" (gold), \"P\" (pit)\n",
        "\n",
        "    String format consumed looks like\n",
        "    0.7 0.2 0.1   [probability of going 1, 2, 3 spaces]\n",
        "    - - - - - - P - - - -   [space-delimited map rows]\n",
        "    - - G - - - - - P - -   [G is gold, P is pit]\n",
        "\n",
        "    You can assume the maps are rectangular, although this isn't enforced\n",
        "    by this constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, probstring):\n",
        "        \"\"\" Consume string formatted as above\"\"\"\n",
        "        self.map = []\n",
        "        for i, line in enumerate(probstring.splitlines()):\n",
        "            if i == 0:\n",
        "                self.move_probs = [float(s) for s in line.split()]\n",
        "            else:\n",
        "                self.map.append(line.split())\n",
        "\n",
        "    def solve(self, iterations, use_q):\n",
        "        \"\"\" Wrapper for MDP and Q solvers.\n",
        "\n",
        "        Args:\n",
        "            iterations (int):  Number of iterations (but these work differently for the two solvers)\n",
        "            use_q (bool):  False means use MDP value iteration, true means use Q-learning\n",
        "        Returns:\n",
        "            A Policy, in either case (what to do in each square; see class below)\n",
        "        \"\"\"\n",
        "\n",
        "        if use_q:\n",
        "            return q_solve(self, iterations)\n",
        "        return mdp_solve(self, iterations)\n",
        "\n",
        "class Policy:\n",
        "    \"\"\" Abstraction on the best action to perform in each state.\n",
        "\n",
        "    This is a string list-of-lists map similar to the problem input, but a character gives the best\n",
        "    action to take in each non-reward square (see MOVES constant at top of file).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, problem):\n",
        "        \"\"\"Args:\n",
        "\n",
        "        problem (Problem):  The MDP problem this is a policy for\n",
        "        \"\"\"\n",
        "        self.best_actions = copy.deepcopy(problem.map)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Join the characters in the policy into one big space-separated, multline string\"\"\"\n",
        "        return '\\n{}\\n'.format('\\n'.join([' '.join(row) for row in self.best_actions]))\n",
        "\n",
        "def roll_steps(move_probs, row, col, move, rows, cols):\n",
        "    \"\"\"Calculates the new coordinates that result from a move.\n",
        "\n",
        "    Includes the \"roll of the dice\" for transition probabilities and checking arena boundaries.\n",
        "\n",
        "    Helper for try_policy and q_solve - probably useful in your Q-learning implementation.\n",
        "\n",
        "    Args:\n",
        "        move_probs (List[float]):  Transition probabilities for the ice (from problem)\n",
        "        row, col (int, int):  location of agent before moving\n",
        "        move (string):  The direction of move as a MOVES character (not an int constant!)\n",
        "        rows, cols (int, int):  number of rows and columns in the map\n",
        "\n",
        "    Returns:\n",
        "        new_row, new_col (int, int):  The new row and column after moving\n",
        "    \"\"\"\n",
        "    displacement = 1\n",
        "    total_prob = 0\n",
        "    move_sample = random.random()\n",
        "    for p, prob in enumerate(move_probs):\n",
        "        total_prob += prob\n",
        "        if move_sample <= total_prob:\n",
        "            displacement = p+1\n",
        "            break\n",
        "    # Handle \"slipping\" into edge of map\n",
        "    new_row = row\n",
        "    new_col = col\n",
        "    if not isinstance(move, str):\n",
        "        print(\"Warning: roll_steps wants str for move, got a different type\")\n",
        "    if move == \"U\":\n",
        "        new_row -= displacement\n",
        "        if new_row < 0:\n",
        "            new_row = 0\n",
        "    elif move == \"R\":\n",
        "        new_col += displacement\n",
        "        if new_col >= cols:\n",
        "            new_col = cols-1\n",
        "    elif move == \"D\":\n",
        "        new_row += displacement\n",
        "        if new_row >= rows:\n",
        "            new_row = rows-1\n",
        "    elif move == \"L\":\n",
        "        new_col -= displacement\n",
        "        if new_col < 0:\n",
        "            new_col = 0\n",
        "    return new_row, new_col\n",
        "\n",
        "\n",
        "def try_policy(policy, problem, iterations):\n",
        "    \"\"\"Returns average utility per move of the policy.\n",
        "\n",
        "    Average utility is as measured by \"iterations\" random drops of an agent onto empty\n",
        "    spaces, running until gold, pit, or time limit MAX_MOVES is reached.\n",
        "\n",
        "    Doesn't necessarily play a role in your code, but you can try policies this\n",
        "    way\n",
        "\n",
        "    Args:\n",
        "        policy (Policy):  the policy the agent is following\n",
        "        problem (Problem):  the environment description\n",
        "        iterations (int):  the number of random trials to run\n",
        "    \"\"\"\n",
        "    total_utility = 0\n",
        "    total_moves = 0\n",
        "    for _ in range(iterations):\n",
        "        # Resample until we have an empty starting square\n",
        "        while True:\n",
        "            row = random.randrange(0, len(problem.map))\n",
        "            col = random.randrange(0, len(problem.map[0]))\n",
        "            if problem.map[row][col] == \"-\":\n",
        "                break\n",
        "        for moves in range(MAX_MOVES):\n",
        "            total_moves += 1\n",
        "            policy_rec = policy.best_actions[row][col]\n",
        "            # Take the move - roll to see how far we go, bump into map edges as necessary\n",
        "            row, col = roll_steps(problem.move_probs, row, col, policy_rec, \\\n",
        "                                  len(problem.map), len(problem.map[0]))\n",
        "            if problem.map[row][col] == \"G\":\n",
        "                total_utility += GOLD_REWARD\n",
        "                break\n",
        "            if problem.map[row][col] == \"P\":\n",
        "                total_utility += PIT_REWARD\n",
        "                break\n",
        "    return total_utility / total_moves\n",
        "\n",
        "\n",
        "#helper function returns the new coordinates of a move\n",
        "#accounts for cases where the distance goes off map-- insteas returns endge position\n",
        "def get_pos(row,col,move,distance,problem):\n",
        "    new_row = row\n",
        "    new_col = col\n",
        "    rows = len(problem.map)\n",
        "    cols = len(problem.map[0])\n",
        "\n",
        "    if move == \"U\":\n",
        "        new_row -= distance\n",
        "        if new_row < 0:\n",
        "            new_row = 0\n",
        "    elif move == \"R\":\n",
        "        new_col += distance\n",
        "        if new_col >= cols:\n",
        "            new_col = cols-1\n",
        "    elif move == \"D\":\n",
        "        new_row += distance\n",
        "        if new_row >= rows:\n",
        "            new_row = rows-1\n",
        "    elif move == \"L\":\n",
        "        new_col -= distance\n",
        "        if new_col < 0:\n",
        "            new_col = 0\n",
        "    return new_row, new_col\n",
        "\n",
        "#Utility update function\n",
        "def updateUtil(moves, problem,rewards,utils,r,c):\n",
        "    best_u = -1000\n",
        "    best_m = -1\n",
        "    #Iterate through all possible moves\n",
        "    for move in moves:\n",
        "        curr = 0\n",
        "        step = 1\n",
        "        #Iterate through all possible distances and calculate expected utility for move\n",
        "        while step <= len(problem.move_probs):\n",
        "            new_r, new_c = get_pos(r,c,MOVES[move],step,problem)\n",
        "            curr+= (problem.move_probs[step-1] * (DISCOUNT_FACTOR * utils[new_r][new_c]+rewards[new_r][new_c]))\n",
        "            step+=1\n",
        "\n",
        "        #Only keep move with best utility- thats the one which represents the square\n",
        "        if curr> best_u:\n",
        "            best_u = curr\n",
        "            best_m = move\n",
        "\n",
        "\n",
        "    return best_u, best_m\n",
        "\n",
        "\n",
        "\n",
        "def mdp_solve(problem, iterations):\n",
        "    \"\"\" Perform value iteration for the given number of iterations on the MDP problem.\n",
        "\n",
        "    Here, the squares with rewards can be initialized to the reward values, since value iteration\n",
        "    assumes complete knowledge of the environment and its rewards.\n",
        "\n",
        "    Args:\n",
        "        problem (Problem):  description of the environment\n",
        "        iterations (int):  number of complete passes over the utilities\n",
        "    Returns:\n",
        "        a Policy (though you may design this to return utilities as a second return value)\n",
        "    \"\"\"\n",
        "    r = len(problem.map)\n",
        "    c = len(problem.map[0])\n",
        "    utils = []\n",
        "    rewards = []\n",
        "\n",
        "    #Initializing utils and rewards before start\n",
        "    for i in range(r):\n",
        "        r_row = []\n",
        "        for j in range(c):\n",
        "            if problem.map[i][j]==\"G\":\n",
        "                r_row.append(GOLD_REWARD)\n",
        "            elif problem.map[i][j]==\"P\":\n",
        "                r_row.append(PIT_REWARD)\n",
        "            else:\n",
        "                r_row.append(0.0)\n",
        "        rewards.append(r_row)\n",
        "\n",
        "    utils = copy.deepcopy(rewards)\n",
        "\n",
        "\n",
        "\n",
        "    res = copy.deepcopy(problem)\n",
        "    #Value iteration for eech cell\n",
        "    for _ in range(ITERATIONS):\n",
        "        new_utils = copy.deepcopy(utils)\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                #Terminal state if G or P\n",
        "                if (problem.map[i][j]==\"G\" or problem.map[i][j]==\"P\"):\n",
        "                    continue\n",
        "                mvs = get_pos_moves(problem,i,j)\n",
        "                #Update util with possible moves, and return best move\n",
        "                new_utils[i][j],best_move = updateUtil(mvs,problem,rewards,utils,i,j)\n",
        "                res.map[i][j] = MOVES[best_move]\n",
        "        utils = new_utils\n",
        "\n",
        "    # TODO calculate the policy\n",
        "    policy = Policy(res)\n",
        "    return policy\n",
        "\n",
        "def q_solve(problem, iterations):\n",
        "    \"\"\"q_solve:  Use Q-learning to find a good policy on an MDP problem.\n",
        "\n",
        "    Each iteration corresponds to a random drop of the agent onto the map, followed by moving\n",
        "    the agent until a reward is reached or MAX_MOVES moves have been made.  When an agent\n",
        "    is sitting on a reward, update the utility of each move from the space to the reward value\n",
        "    and end the iteration.  (For simplicity, the agent also does this if just dropped there.)\n",
        "    The agent does not \"know\" reward locations in its Q-values before encountering the space\n",
        "    and \"discovering\" the reward.\n",
        "\n",
        "    Note that texts differ on when to pay attention to this reward - this code follows the\n",
        "    convention of scoring rewards of the space you are moving *from*, plus discounted best q-value\n",
        "    of where you landed.\n",
        "\n",
        "    Assume epsilon-greedy exploration.  Leave reward letters as-is in the policy,\n",
        "    to make it more readable.\n",
        "\n",
        "    Args:\n",
        "        problem (Problem):  The environment\n",
        "        iterations (int):  The number of runs from random start to reward encounter\n",
        "    Returns:\n",
        "        A Policy for the map\n",
        "    \"\"\"\n",
        "    r = len(problem.map)\n",
        "    c = len(problem.map[0])\n",
        "    rewards = []\n",
        "    qVals = []\n",
        "    #Initializing Qvals arr and rewards\n",
        "    for i in range(r):\n",
        "        q_row = []\n",
        "        r_row = []\n",
        "        for j in range(c):\n",
        "            q_row.append([0.0,0.0,0.0,0.0])\n",
        "            if problem.map[i][j]==\"G\":\n",
        "                r_row.append(GOLD_REWARD)\n",
        "            elif problem.map[i][j]==\"P\":\n",
        "                r_row.append(PIT_REWARD)\n",
        "            else:\n",
        "                r_row.append(0.0)\n",
        "        qVals.append(q_row)\n",
        "        rewards.append(r_row)\n",
        "\n",
        "    #Learning\n",
        "    for _ in range(ITERATIONS):\n",
        "        #Random drop on the map\n",
        "        r_drop = random.randint(0,r-1)\n",
        "        c_drop = random.randint(0,c-1)\n",
        "        mvs = 0\n",
        "\n",
        "        while mvs < MAX_MOVES:\n",
        "            #If we get dropped on p or g, its a terminal state and we end trial.\n",
        "            if problem.map[r_drop][c_drop] == \"P\":\n",
        "                qVals[r_drop][c_drop] = [PIT_REWARD, PIT_REWARD, PIT_REWARD, PIT_REWARD]\n",
        "                break\n",
        "\n",
        "            if problem.map[r_drop][c_drop] == \"G\":\n",
        "                qVals[r_drop][c_drop] = [GOLD_REWARD, GOLD_REWARD, GOLD_REWARD, GOLD_REWARD]\n",
        "                break\n",
        "\n",
        "            pr = random.random()\n",
        "            possible_moves = get_pos_moves(problem,r_drop,c_drop)\n",
        "            action = 0\n",
        "            #WIth prob EXPLORE_PROB we pick a random possible move, otherwise we pick best q\n",
        "            if pr < EXPLORE_PROB:\n",
        "                action = random.choice(possible_moves)\n",
        "            else:\n",
        "                possibe_qs = [qVals[r_drop][c_drop][x] for x in possible_moves]\n",
        "                best_action_q = max(possibe_qs)\n",
        "                action = qVals[r_drop][c_drop].index(best_action_q)\n",
        "\n",
        "            #Calling role step function using the move we chose to get new coordinates\n",
        "            new_row,new_col = roll_steps(problem.move_probs,r_drop,c_drop,MOVES[action],r,c)\n",
        "            #calling new_q to update current q\n",
        "            q = new_q(rewards,qVals,r_drop,c_drop,new_row,new_col,action)\n",
        "            #Updating q in qvals arr\n",
        "            qVals[r_drop][c_drop][action] = q\n",
        "            #Moving on if not terminal state\n",
        "            r_drop = new_row\n",
        "            c_drop = new_col\n",
        "            mvs+=1\n",
        "\n",
        "\n",
        "    policy = Policy(problem)\n",
        "\n",
        "    for i in range (r):\n",
        "        for j in range(c):\n",
        "            if problem.map[i][j]==\"G\" or  problem.map[i][j]==\"P\":\n",
        "                continue\n",
        "            best_action_index = qVals[i][j].index(max(qVals[i][j]))\n",
        "            best_action = MOVES[best_action_index]\n",
        "            policy.best_actions[i][j] = best_action\n",
        "\n",
        "\n",
        "    return policy\n",
        "\n",
        "#Helper function to return the possible moves from a specific square (to avoid going off map)\n",
        "def get_pos_moves(problem,r,c):\n",
        "    moves = []\n",
        "    if r > 0:\n",
        "        moves.append(0)\n",
        "    if r < len(problem.map) - 1:\n",
        "        moves.append(2)\n",
        "    if c > 0:\n",
        "        moves.append(3)\n",
        "    if c < len(problem.map[0]) - 1:\n",
        "        moves.append(1)\n",
        "    return moves\n",
        "\n",
        "#Helper function to get the reward for a coordinate\n",
        "def get_reward(problem, r, c):\n",
        "    if problem.map[r][c] == \"G\":\n",
        "        return GOLD_REWARD\n",
        "    elif problem.map[r][c] == \"P\":\n",
        "        return PIT_REWARD\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "#Q learning function uses\n",
        "def new_q(rewards, utilities, r, c, new_r, new_c, movenum):\n",
        "    \"\"\" Q-learning function.  Returns the new Q-value for space (r,c).\n",
        "    It's recommended you code and test this before doing the overall Q-learning.\n",
        "\n",
        "    Should use the LEARNING_RATE and DISCOUNT_FACTOR.\n",
        "\n",
        "    Args:\n",
        "        rewards (List[List[float]]):  Reward amounts built into the problem map (indexed [r][c])\n",
        "        utilities (List[List[List[float]]]):  The Q-values for each action from each space.\n",
        "                                              (Indexed as [row][col][move])\n",
        "        r, c (int, int):  Row and column of our location before move\n",
        "        new_r, new_c (int, int):  Row and column of our location after move\n",
        "        movenum (int):  Integer index into the Q-values, corresponding to constants UP etc\n",
        "    Returns:\n",
        "        float - the new Q-value for the space we moved from\n",
        "    \"\"\"\n",
        "    old_q = utilities[r][c][movenum]\n",
        "    action_reward = rewards[new_r][new_c]\n",
        "    best_next_q = max(utilities[new_r][new_c])\n",
        "\n",
        "    new_q = old_q + LEARNING_RATE * (action_reward + DISCOUNT_FACTOR * best_next_q - old_q)\n",
        "\n",
        "\n",
        "    return new_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5cuaEcLvAoYK"
      },
      "outputs": [],
      "source": [
        "deterministic_test = \"\"\"1.0\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ABkxRiTA4Wi"
      },
      "outputs": [],
      "source": [
        "# Notice that we counterintuitively are most likely to go 2 spaces here\n",
        "very_slippy_test = \"\"\"0.2 0.7 0.1\n",
        "- - P - -\n",
        "- - G P -\n",
        "- - P - -\n",
        "- - - - -\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Lqg8ZZUCBYYl"
      },
      "outputs": [],
      "source": [
        "big_test = \"\"\"0.6 0.3 0.1\n",
        "- P - G - P - - G -\n",
        "P G - P - - - P - -\n",
        "P P - P P - P - P -\n",
        "P - - P P - - - - P\n",
        "- - - - - - - - P G\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sHZ99I9uBmiH",
        "outputId": "e19c4966-2a04-4e80-ad17-0c20ae257c5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "D D P R D\n",
            "R R G P D\n",
            "U U P D D\n",
            "U U L L L\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MDP value iteration tests\n",
        "print(Problem(deterministic_test).solve(ITERATIONS, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "txLGS4pUwhh7",
        "outputId": "56bf1105-26d8-4b3e-93e6-4dbb17498d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "D D P R D\n",
            "R R G P D\n",
            "U U P D D\n",
            "U U L L L\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(Problem(sampleMDP).solve(ITERATIONS, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WnprAX2uwiDI",
        "outputId": "14d8e82a-3794-40cf-c710-d336ac441baa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "D L P R D\n",
            "R L G P L\n",
            "D D P D D\n",
            "U L U L U\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(Problem(very_slippy_test).solve(ITERATIONS, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "INhKxA6twic8",
        "outputId": "4ef7709d-2dbc-4080-f61e-9eaea4dbd75d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "R P R G L P R R G L\n",
            "P G U P U R U P U U\n",
            "P P U P P D P D P U\n",
            "P D U P P D D D L P\n",
            "R R U L L L L L P G\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(Problem(big_test).solve(ITERATIONS, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LfUJKMPtCRCs",
        "outputId": "617c8759-9aaa-42de-ce93-75908a5f151c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "R D P R D\n",
            "R R G P D\n",
            "U U P D D\n",
            "U U L L L\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q-learning tests\n",
        "# Set seed every time for consistent executions;\n",
        "# comment out to get different random runs\n",
        "random.seed(340)\n",
        "print(Problem(deterministic_test).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "08cHCoI6wqak",
        "outputId": "590cbad2-611d-43ca-913f-95678e3b2c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "D D P R D\n",
            "R R G P D\n",
            "U U P D D\n",
            "U U L L L\n",
            "\n"
          ]
        }
      ],
      "source": [
        "random.seed(340)\n",
        "print(Problem(sampleMDP).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PMM3kelxwqsx",
        "outputId": "bc06f9e5-e575-4ff9-df4a-97fa24aa896e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "D L P R D\n",
            "R L G P L\n",
            "D L P R D\n",
            "U L U R U\n",
            "\n"
          ]
        }
      ],
      "source": [
        "random.seed(340)\n",
        "print(Problem(very_slippy_test).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hWu_w30AwrP9",
        "outputId": "34b4fff2-5062-49d0-a61c-31ea7ee30d22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "R P R G L P R R G L\n",
            "P G U P U D U P U U\n",
            "P P U P P D P D P U\n",
            "P D U P P R D D L P\n",
            "R R U L L L L L P G\n",
            "\n"
          ]
        }
      ],
      "source": [
        "random.seed(340)\n",
        "print(Problem(big_test).solve(ITERATIONS, True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbP5VrvF9dIt"
      },
      "source": [
        "Once you're done, here are a few thought questions (19 points total):\n",
        "\n",
        "**3, 5 points) Suppose we are on the deterministic map where there is no sliding on ice, and performing value iteration until it converges.  Supposing 0 < DISCOUNT_FACTOR < 1, how does the policy change if the discount factor changes to another value in that range (or does the policy change at all)?  Why does that happen?  What happens to the policy if DISCOUNT_FACTOR = 1?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzD5X8wY-5gS"
      },
      "source": [
        "**TODO**\n",
        "\n",
        "\n",
        "The discount factor affects how much the value of utils/rewards is reduced as time/distance from the reward is increased. A low discount factor will cause the agent to follow paths with more immediate rewards, because paths with several moves until a reward is rached will be exponentially small due to the discount factor being small and being multiplied several times. On the other hand, a high discount factor will favor/allow the agent to explore paths where rewards are far down the line. If discount_facor is 1, the policy  will be wrong (tested in our case above, wrong results given) because upon the first the agent will assume it reached terminal reward states as value iteration updates all adjacent squares next to gold/pits.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CkLwZnX-87e"
      },
      "source": [
        "**4, 3 points) The value iteration MDP solver updates all squares an equal number of times.  The Q-learner does not.  Which squares might we expect the Q-learner to update the most?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEhHogqO_7fC"
      },
      "source": [
        "**TODO**\n",
        "\n",
        "At the start of the q-learning trial,the agent will explore the grid randomly due to the empty q-table and random drops, however as the learning progresses, the q-learner will tend to move to squares with higher q values. Therfore, we can expect that in the long run the q-learner will update squares in/around the rewards most often."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeGcHwKR_-m7"
      },
      "source": [
        "**5, 11 points) Suppose we change the state information so that, instead of knowing its coordinates on the map, the agent instead knows just the locations of all rewards in a 5x5 square with the agent at the square center.  Thus, at the start of every run, it may not know exactly where it is, but it knows what is in the vicinity.  It also does not know the transition model.**\n",
        "\n",
        "**a, 2 points) We can't use value iteration here.  Why?**\n",
        "\n",
        "**b, 4 points) How many state-action combinations are possible, assuming the contents of the agent's own square don't matter, and every other square could have a pit, gold, or an empty square as in the example maps?  Is a lookup table of Q-values feasible if we allocate memory for each possible state-action combination?  (Let's define \"feasible\" as \"able to be stored in a gig or less of memory,\" assuming 64-bit values.)**\n",
        "\n",
        "**c, 5 points) Let's suppose we want to instead generate Q-values with a classic neural network with a single hidden layer.  The inputs are the contents of the 24 squares in the 5x5 square that the player is not  in (we can encode gold = 1, nothing = 0, pit = -1).  There are 10 hidden units.  There are 4 output units corresponding to the 4 possible actions' Q-values.  How much memory is required for the weights of this network, assuming each is a 32-bit float (don't forget bias weights for each unit)?  Comparing to part (b), is it more efficient in memory to use a lookup table for Q(s,a), or this neural network?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inb8brIUIk8U"
      },
      "source": [
        "**a) TODO**\n",
        "\n",
        "value iteration requires us to know the rewards/state of the entire grid, not jus the surrounding squares. Value iteration is not possible in this scenario because once we try to update the squares at the endge of our 5x5, we don't have information about the squares outside of it so we cannot apply bellmans equation.\n",
        "\n",
        "**b) TODO**\n",
        "\n",
        "Since there are 24 squares in the 5x5 excluding the square we are in and 3 possible rewards in each square, this gives us\n",
        "\n",
        "3*3*3*... = 3^24 reward layouts/state.\n",
        "\n",
        "Since there are 4 actions, this gives us 3^24*4 state-action combinations. if each combination is 64-bits, this gives us\n",
        "\n",
        " 3^24*4*64 = 7.2301961e+13 bits = 9037.745 gigabytes. therefore it is unfeasible.\n",
        "\n",
        "\n",
        "\n",
        "**c) TODO**\n",
        "\n",
        "in the neural network case we have (25-1 = 24) input nodes, 10 hidden nodes, and 4 output nodes (each action).\n",
        "The total number of weights is:\n",
        "24*10(input-hidden)+10(bias) = 250\n",
        "10*4(hidden-output)+4(bias) = 44\n",
        "total = 294\n",
        "\n",
        "Since each weight is 32-bits, the total number of bits is:\n",
        "\n",
        "294*32 = 9408bits, which is singificantly less than a gig. Comparing with part-b, it seems much more efficient to use a nearual network instead of a full q-lookup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78PjFoZBOLfp"
      },
      "source": [
        "**Remember to submit your code on Blackboard as both an .ipynb (File->Download->.ipynb) and a PDF (Print->Save as PDF).**"
      ]
    }
  ],
  "metadata": {
    "author": [
      {
        "@type": "Person",
        "name": "Kevin Gold"
      }
    ],
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
